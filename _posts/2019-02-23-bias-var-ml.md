---
layout: post
title: ML的方差与偏差
category: 算法
comments: true
description: ML的方差与偏差
keywords: ML的方差与偏差
---

偏差-方差分解可以解释学习器的泛化性能。



偏差与方差分别是用于衡量一个模型泛化误差的两个方面；

* 模型的偏差，指的是模型预测的期望值与真实值之间的差；

* 模型的方差，指的是模型预测的期望值与预测值之间的差平方和；

在监督学习中，模型的泛化误差可分解为偏差、方差与噪声之和。

$$ Err(x) = Bias^2(x) +var(x) + NoiseError $$


**偏差**用于描述模型的**拟合能力**；**方差**用于描述模型的**稳定性**。


### 导致偏差和方差的原因

当模型错误，特征不足，或者训练样本少的情况下，模型输出与实际输出有很大的偏差。此时方差比较小（因为不管怎么选择样本，输出结果都很大的偏离正确结果）

当特征选取太多，模型复杂度过度的时候，偏差较小，方差比较大。不同的测试数据，测试结果会有很大的波动。

### 如何权衡方差偏差，找到合适的模型

一般情况，偏差方差是相互冲突的。偏差和方差的关系和模型容量（模型复杂度)相关，不同的时候导致欠拟合，过拟合的结果。

可以通过绘制学习曲线，找到方差和偏差相对平衡的模型训练程度。


在使用cross-validation的时候，如果K值选取比较高，则会使得模型过度拟合测试集，bias比较低，但是var比较高。


### Bagging与Boosting在方差偏差上的区别

Bagging：多个分类器，对原样本进行有放回的抽样训练，组成集成分类器，然后使用投票等算法得到最终结果。其代表算法为随机森林。

Boosting：使用一系列分类器，对每个子分类器输出的错误值增加权重，使得直到结束。其代表算法为AdaBoost、GBDT、XGBoost。

对于Bagging而言，使用了多个不同的分类器，降低整个集成分类器的方差。对于基分类器而言，其目的更多是降低偏差，会偏向于更加复杂的分类器。

对于Boosting而言，每次都会对错误结构增加权重，因此降低了整个集成分类器的偏差。对于基分类器而言，其目的是降低方差。因此会尽可能选择简单分类器。

因此，一般情况，随机森林的树的深度往往大于GBDT的树的深度。

