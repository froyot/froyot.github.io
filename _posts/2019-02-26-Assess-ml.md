---
layout: post
title: ML算法评估标准
category: 算法
comments: true
description: ML算法评估标准
keywords: 机器学习,查准率,查全率,准确率,召回率,精确率，错误率,ROC,f1曲线,AUC,AOC
---


机器学习中，模型评价标准太多了，根本分不清谁是谁，统一整理放在一篇文章当中，方便以后查阅

|实际类别|预测正|预测负|
|:--|:--|:--|
|正|TP|FN|
|负|FP|TN|


### 错误率

错误率表示分类错误的比例(包括正样本分类为负样本FN,负样本分类为正样本FP)

### 精度,准确率（ACC）

分类正确的比例(TP+TN) = 1-错误率

### 精确率，查准率（P）

查全率表示有多少正样本被正确找出(TP/(TP+TP) )，衡量查询结果信噪比，表征查找的准确性

### 查全率，召回率(R)

查准率表示查找出来的正样本占实际正样本的比例(TP/(TP+FN)),表征查找的全面性


### ROC	曲线

ROC曲线的横坐标为false positive rate（FPR）：FP/(FP+TN)


纵坐标为true positive rate（TPR）：TP/(TP+FN)


(0,1)点，即FPR=0,TPR=1,这是一个完美的分类器，它将所有的样本都正确分类。

(1,0)点，即FPR=1，TPR=0，这是一个最糟糕的分类器，因为它成功避开了所有的正确答案。

(0,0)点，即FPR=TPR=0，即FP（false positive）=TP（true positive）=0，全都没被检测到，即全部选0

（1,1）点，分类器实际上预测所有的样本都为1 。

分析可只：ROC曲线越接近左上角，该分类器的性能越好。如果ROC曲线完全被另一个分类器包裹，则任务另一个分类器的性能优于当前分类器

ROC曲线有个很好的特性：当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。


### AUC

AUC（Area Under Curve）被定义为ROC曲线下的面积，完全随机的二分类器的AUC为0.5

AUC反应的是分类器对样本的排序能力


### F1

F1=2\*(P\*R)/(P+R)

P是查准率，R是查全率

F1兼顾了分类模型的准确率和召回率，可以看作是模型准确率和召回率的调和平均数，最大值是1，最小值是0。F1越大越好




